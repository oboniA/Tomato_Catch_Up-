# -*- coding: utf-8 -*-
"""MSc_final_project_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TDousKvYr_4-dGnEgEkeZsdUotvLuwlz

**Hardware Accelerator: L4 GPU**

# Libraries and imports
"""

from google.colab import drive
import os
import zipfile
from sklearn.model_selection import train_test_split
import shutil
import glob
import cv2
import numpy as np
from sklearn import preprocessing
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.vgg16 import VGG16
import keras
from keras.models import Sequential
from keras.layers import Flatten, Dense, Dropout
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
!pip install keras-tuner
import keras_tuner as kt
import time
from keras import regularizers
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from google.colab import files
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import tensorflow as tf

"""# Data Preparation

**Import dataset from google drive**

dataset in google drive is a zip folder. Here, the folder is unzipped and loaded into collab.
"""

drive.mount('/content/drive')

zip_file = '/content/drive/MyDrive/TomatoPlantVillage.zip'
extracted_file = '/content/PlantVillage'

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall(extracted_file)

os.listdir(extracted_file)
work_folder = '/content/PlantVillage/TomatoPlantVillage'
os.listdir(work_folder)

"""**Create training, validation, and test sets directories**"""

#destinations
split_dataset = "TomatoPlantDataset/"
train_path = os.path.join(split_dataset, "training_data")
val_path = os.path.join(split_dataset, "validation_data")
test_path = os.path.join(split_dataset, "testing_data")

# makes seperate directories
os.makedirs(train_path, exist_ok=True)
os.makedirs(test_path, exist_ok=True)

"""**Data split**

Training 70%
Validation 15%
Testing 15%
"""

train_ratio = 0.70
val_ratio = 0.15
test_ratio = 0.15

# function to split data
def datasplit(work_folder, train_path, val_path, test_path):

    # gets class names
    class_names = os.listdir(work_folder)
    for cls in class_names:
        class_dir = os.path.join(work_folder, cls)
        images = os.listdir(class_dir)

        # splits dataset
        train_data, test_data = train_test_split(images, test_size=test_ratio, random_state=42)
        train_val, val_data = train_test_split(train_data, test_size=val_ratio/(train_ratio+val_ratio), random_state=42)

        # copies images to dedicated directories
        for img in train_val:
            shutil.copy(os.path.join(class_dir, img), os.path.join(train_path, cls))
        for img in val_data:
            shutil.copy(os.path.join(class_dir, img), os.path.join(val_path, cls))
        for img in test_data:
            shutil.copy(os.path.join(class_dir, img), os.path.join(test_path, cls))

# create subdirectories for each class in train, val, test directories
for cls in os.listdir(work_folder):
    os.makedirs(os.path.join(train_path, cls), exist_ok=True)
    os.makedirs(os.path.join(val_path, cls), exist_ok=True)
    os.makedirs(os.path.join(test_path, cls), exist_ok=True)

# data split
datasplit(work_folder, train_path, val_path, test_path)

"""**Check data distribution in destination folders**"""

def calculate_and_print_splits(train_dir, val_dir, test_dir):
    total_train = sum([len(files) for _, _, files in os.walk(train_dir)])
    total_val = sum([len(files) for _, _, files in os.walk(val_dir)])
    total_test = sum([len(files) for _, _, files in os.walk(test_dir)])

    total_files = total_train + total_val + total_test

    print(f"Total Files: {total_files}")
    print(f"Training Files: {total_train} ({(total_train / total_files) * 100:.2f}%)")
    print(f"Validation Files: {total_val} ({(total_val / total_files) * 100:.2f}%)")
    print(f"Testing Files: {total_test} ({(total_test / total_files) * 100:.2f}%)")

calculate_and_print_splits(train_path, val_path, test_path)

folder_path = 'data/testdata'
zip_path = 'testdata.zip'
shutil.make_archive('testdata', 'zip', folder_path)

"""# Data augmentation and prepare training and validation pipelines"""

training_dir = '/content/TomatoPlantDataset/training_data'
validation_dir = '/content/TomatoPlantDataset/validation_data'
testing_dir = '/content/TomatoPlantDataset/testing_data'

"""**Download the test data beforehand for testing purpose!**"""

test_zip = '/content/testdata.zip'
shutil.make_archive('testdata', 'zip', testing_dir)

files.download(test_zip)

"""**Augmentation**"""

# https://github.com/amitjha11/Plant-Disease-Detection-Using-VGG16/blob/master/plant-disease-detection-using-vgg16.ipynb

SIZE = 224  # in pixels
batch_size = 128

# data augmentation for training data
train_datagenerator = ImageDataGenerator(rescale=1./255,
                                          width_shift_range=0.2,
                                          height_shift_range=0.2,
                                          shear_range=0.2,
                                          zoom_range=0.2,
                                          fill_mode='nearest')

# normalization for validation and test data
val_datagenerator = ImageDataGenerator(rescale=1.0 / 255)
test_datagenerator = ImageDataGenerator(rescale=1./255)

# generators
train_gen = train_datagenerator.flow_from_directory(training_dir, target_size=(SIZE, SIZE), batch_size=batch_size, class_mode='categorical', shuffle=True)
val_gen = val_datagenerator.flow_from_directory(validation_dir, target_size=(SIZE, SIZE), batch_size=batch_size, class_mode='categorical', shuffle=False)
test_gen = test_datagenerator.flow_from_directory(testing_dir, target_size=(SIZE, SIZE), batch_size=batch_size, class_mode='categorical', shuffle=False)

train_samples = train_gen.samples
val_samples = val_gen.samples

print(f"Training samples: {train_samples}")
print(f"Validation samples: {val_samples}")

"""**Extract Numerical classes from Categorical classes**

for confuson matrix
"""

class_names = train_gen.class_indices
class_keys = list(class_names.keys())

print(class_names)
print(class_keys)

"""# **Class Oversampling**"""

# https://stackoverflow.com/questions/69783897/compute-class-weight-function-issue-in-sklearn-library-when-used-in-keras-cl
# https://scikit-learn.org/dev/modules/generated/sklearn.utils.class_weight.compute_class_weight.html

compute_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_gen.classes), y=train_gen.classes)
class_weights = dict(zip(np.unique(train_gen.classes), compute_weights))
class_weights

"""# Feature Extraction

**Set up VGG16 as feature extractor**
"""

# https://stackoverflow.com/questions/65851403/vgg16-with-other-input-shape-and-imagenet-weights
# https://www.youtube.com/watch?v=Fxy6WTnUIww

# VGG16 model with pre-trained weights and without fully connected layer
vgg16_feature_extractor = VGG16(include_top=False,
                   input_shape=(SIZE, SIZE, 3),
                   weights='imagenet')

# loaded layers as non-trainable, freezing
for layer in vgg16_feature_extractor.layers:
	  layer.trainable = False  # freeze VGG16 layers initially

vgg16_feature_extractor.summary()

# https://towardsdatascience.com/fine-tuning-pre-trained-model-vgg-16-1277268c537f

for i, layer in enumerate(vgg16_feature_extractor.layers):
    print(i, layer.name, layer.trainable)

"""# Hyperparameter Tuning

**Hyperparameter tuning Function**
"""

# https://www.digitalocean.com/community/tutorials/hyperparameter-optimization-with-keras-tuner
# hp.Int() https://medium.com/swlh/hyperparameter-tuning-in-keras-tensorflow-2-with-keras-tuner-randomsearch-hyperband-3e212647778f
# https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/
# https://www.kaggle.com/code/mukeshmanral/using-kerastuner-dropout

def custom_hyp_tune(hp):

    custom_hp = keras.models.Sequential()
    custom_hp.add(vgg16_feature_extractor)
    custom_hp.add(Flatten())
    custom_hp.add(Dense(hp.Int('dense_units', min_value=8, max_value=100, step=20),
                        activation='relu',
                        kernel_regularizer=regularizers.l2(0.01)))
    custom_hp.add(Dropout(hp.Choice(name='dropout', values=[0.25, 0.5])))
    custom_hp.add(Dense(10, activation='softmax'))

    custom_hp.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Float('learning_rate',
                                                                              min_value=1e-5, max_value=1e-2,
                                                                              sampling='log')),
                    loss='categorical_crossentropy', metrics=['accuracy'])

    return custom_hp

""".search() is to perform hyperparameter tuning using Keras Tuner to find the best set of hyperparameters.

performs a randomized search over the defined hyperparameter space, to identify the model configuration that provides the best validation accuracy.

**Optimization with Random Search**
"""

# https://medium.com/@piyushkashyap045/mastering-hyperparameter-tuning-for-neural-networks-with-keras-tuner-c0b64d549188#:~:text=Keras%20Tuner%20is%20a%20library,tuning%20for%20deep%20learning%20models.

# hyperparameters
k_tuner = kt.RandomSearch(
    custom_hyp_tune,
    objective='val_accuracy',
    max_trials=5,
    seed=42,
    directory='keras_tuning_directory',
    project_name='hyperparameter_tuning'
)

early_stop = EarlyStopping(monitor='val_loss',
                               patience=2,
                               restore_best_weights=True)

cp = ModelCheckpoint(filepath='custom_best_hyperparameter_model.keras', verbose=1, save_best_only=True)

# random Search
start = time.time()
k_tuner.search(train_gen,
               epochs=5,
               steps_per_epoch = train_samples // batch_size,
               validation_data=val_gen,
               validation_steps = val_samples // batch_size,
               class_weight = class_weights,
               verbose=1,
               callbacks=[cp, early_stop])
end = time.time()
print(f"Search time: {end - start} seconds")

best_hyperparameter = k_tuner.get_best_hyperparameters(1)[0]
best_hyperparameter.values

# to force delete a folder

import tensorflow as tf
!rm -rf /content/keras_tuning_directory

"""# Model Training, validation, Testing

**Function for experiments**
"""

def custom_model_experiments(neurons, dropout, learning_rate):

    custom_model = keras.models.Sequential()
    custom_model.add(vgg16_feature_extractor)
    custom_model.add(Flatten())
    custom_model.add(Dense(neurons, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
    custom_model.add(Dropout(dropout))
    custom_model.add(Dense(10, activation='softmax'))

    custom_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
                    loss='categorical_crossentropy', metrics=['accuracy'])

    return custom_model

"""**Experimental Training**"""

def train_and_evaluate(neurons, dropout, learning_rate, trial_id=1):

    model = custom_model_experiments(neurons, dropout, learning_rate)

    early_stop = EarlyStopping(monitor='val_loss',
                               patience=2,
                               restore_best_weights=True,
                               verbose=1)

    print(f"for neurons={neurons}, dropout={dropout}, learning_rate={learning_rate}:")

    start = time.time()
    best_model_history = model.fit(
        train_gen,
        steps_per_epoch = train_samples // batch_size,
        validation_data = val_gen,
        epochs = 5,
        validation_steps = val_samples // batch_size,
        class_weight = class_weights,
        callbacks=[early_stop],
        verbose=1
        )
    end = time.time()
    print(f"Training time: {end - start} seconds")

    val_accuracy = max(best_model_history.history['val_accuracy'])  # among 5 epoch
    print(f"Highest Validation Accuracy: {val_accuracy}")

    test_loss, test_accuracy = model.evaluate(test_gen, verbose=1)
    print(f"Test Accuracy: {test_accuracy}")

    model_filename = f'Experimental_trial_{trial_id}_model.h5'
    model.save(model_filename)
    print(f"Model for experimental trial {trial_id} saved as {model_filename}")

    return val_accuracy, test_accuracy, model_filename

"""# Tests and Experiments

With different hyperparameter combinations

**Test 1**

Trial 1-6
"""

results = []

results.append(train_and_evaluate(28, 0.5, 0.0007, trial_id=1))
results.append(train_and_evaluate(64, 0.5, 0.0007, trial_id=2))
results.append(train_and_evaluate(128, 0.5, 0.0007, trial_id=3))
results.append(train_and_evaluate(256, 0.5, 0.0007, trial_id=4))
results.append(train_and_evaluate(512, 0.5, 0.0007, trial_id=5))
results.append(train_and_evaluate(1024, 0.5, 0.0007, trial_id=6))

for result in results:
    val_accuracy, test_accuracy, model_filename = result
    print(f"Trial model {model_filename}: Validation Accuracy = {val_accuracy}, Test Accuracy = {test_accuracy}")

"""**Test 2**

Trial 7-12
"""

results2 = []

results2.append(train_and_evaluate(28, 0.25, 0.0007, trial_id=7))
results2.append(train_and_evaluate(64, 0.25, 0.0007, trial_id=8))
results2.append(train_and_evaluate(128, 0.25, 0.0007, trial_id=9))
results2.append(train_and_evaluate(256, 0.25, 0.0007, trial_id=10))
results2.append(train_and_evaluate(512, 0.25, 0.0007, trial_id=11))
results2.append(train_and_evaluate(1024, 0.25, 0.0007, trial_id=12))

for result in results2:
    val_accuracy, test_accuracy, model_filename = result
    print(f"Trial model {model_filename}: Validation Accuracy = {val_accuracy}, Test Accuracy = {test_accuracy}")

"""**Test 3**

Trial 13-16
"""

results3 = []

results3.append(train_and_evaluate(1024, 0.5, 0.007, trial_id=13))
results3.append(train_and_evaluate(1024, 0.5, 0.07, trial_id=14))
results3.append(train_and_evaluate(1024, 0.5, 0.7, trial_id=15))
results3.append(train_and_evaluate(1024, 0.5, 0.00007, trial_id=16))

for result in results3:
    val_accuracy, test_accuracy, model_filename = result
    print(f"Trial model {model_filename}: Validation Accuracy = {val_accuracy}, Test Accuracy = {test_accuracy}")

"""**Test 4**

Trial 17 & 20
"""

results4 = []

results4.append(train_and_evaluate(1024, 0.25, 0.007, trial_id=17))
results4.append(train_and_evaluate(1024, 0.25, 0.07, trial_id=18))
results4.append(train_and_evaluate(1024, 0.25, 0.7, trial_id=19))
results4.append(train_and_evaluate(1024, 0.25, 0.00007, trial_id=20))


for result in results4:
    val_accuracy, test_accuracy, model_filename = result
    print(f"Trial model {model_filename}: Validation Accuracy = {val_accuracy}, Test Accuracy = {test_accuracy}")

"""**Test 5**

Trial 21-24
"""

results5 = []

results5.append(train_and_evaluate(2048, 0.5, 0.00007, trial_id=21))
results5.append(train_and_evaluate(2048, 0.25, 0.00007, trial_id=22))
results5.append(train_and_evaluate(2048, 0.5, 0.0007, trial_id=23))
results5.append(train_and_evaluate(2048, 0.25, 0.0007, trial_id=24))


for result in results5:
    val_accuracy, test_accuracy, model_filename = result
    print(f"Trial model {model_filename}: Validation Accuracy = {val_accuracy}, Test Accuracy = {test_accuracy}")

"""**Test 6**

Trial 25
"""

results6 = []

results6.append(train_and_evaluate(1024, 0, 0.00007, trial_id=25))

for result in results6:
    val_accuracy, test_accuracy, model_filename = result
    print(f"Trial model {model_filename}: Validation Accuracy = {val_accuracy}, Test Accuracy = {test_accuracy}")

"""**Experimental function with higher Epoch and NO early stopping**"""

def train_and_evaluate_improve(neurons, dropout, learning_rate, trial_id=1):

    model = custom_model_experiments(neurons, dropout, learning_rate)

    print(f"for neurons={neurons}, dropout={dropout}, learning_rate={learning_rate}:")

    start = time.time()
    best_model_history = model.fit(
        train_gen,
        steps_per_epoch = train_samples // batch_size,
        validation_data = val_gen,
        epochs = 10,
        validation_steps = val_samples // batch_size,
        class_weight = class_weights,
        verbose=1
        )
    end = time.time()
    print(f"Training time: {end - start} seconds")

    val_accuracy = max(best_model_history.history['val_accuracy'])  # among 10 epoch
    print(f"Highest Validation Accuracy: {val_accuracy}")

    test_loss, test_accuracy = model.evaluate(test_gen, verbose=1)
    print(f"Test Accuracy: {test_accuracy}")

    model_filename = f'Experimental_trial_{trial_id}_model.h5'
    model.save(model_filename)
    print(f"Model for experimental trial {trial_id} saved as {model_filename}")

    return val_accuracy, test_accuracy, model_filename

"""**Test 7**

Trial 26
"""

results7 = []

results7.append(train_and_evaluate_improve(1024, 0, 0.00007, trial_id=26))

"""**Download model**"""

final_model = load_model('/content/drive/My Drive/Experimental_trial_26_model.h5')

"""**Model Architecture**"""

final_model.summary()

"""# Evaluation

**Confusion Matrix**
"""

y_predict = final_model.predict(test_gen)
y_predict = np.argmax(y_predict, axis=1)

cm = confusion_matrix(test_gen.classes, y_predict)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_keys, yticklabels=class_keys)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""**Classification Report**"""

print(classification_report(test_gen.classes, y_predict, target_names=class_keys))